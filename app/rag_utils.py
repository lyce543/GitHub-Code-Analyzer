from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from app.llm import get_openrouter_llm, get_openrouter_llm_streaming
import os
import json

db = None
VECTORSTORE_PATH = "vectorstore"
STATE_FILE = "repo_state.json"

def load_repo_state():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–∞–Ω —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é –∑ —Ñ–∞–π–ª—É"""
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                if state.get("repo_path") and os.path.exists(state["repo_path"]):
                    return state["repo_path"]
        return None
    except Exception as e:
        print(f"‚ùå Error loading state in rag_utils: {e}")
        return None

def load_documents(repo_path):
    from pathlib import Path
    documents = []
    for file in Path(repo_path).rglob("*.py"):
        try:
            with open(file, encoding='utf-8') as f:
                documents.append({"text": f.read(), "path": str(file)})
        except Exception as e:
            print(f"‚ùå Error reading file {file}: {e}")
            continue
    return documents

def index_documents(repo_path):
    global db
    docs = load_documents(repo_path)
    
    if not docs:
        raise Exception("No Python files found in repository")
    
    texts = [doc["text"] for doc in docs]
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.create_documents(texts)
    embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    db = FAISS.from_documents(splits, embedding)

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É
    if os.path.exists(VECTORSTORE_PATH):
        import shutil
        shutil.rmtree(VECTORSTORE_PATH)
    
    db.save_local(VECTORSTORE_PATH)
    print(f"‚úÖ Indexed {len(docs)} files, created {len(splits)} chunks")

def ensure_db_loaded():
    """–ü–µ—Ä–µ–∫–æ–Ω—É—î—Ç—å—Å—è, —â–æ –≤–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞"""
    global db
    
    if db is not None:
        return True
    
    print("üü° FAISS DB not loaded ‚Äî trying to load from disk...")
    
    if not os.path.exists(VECTORSTORE_PATH):
        print(f"‚ùå Vector store path doesn't exist: {VECTORSTORE_PATH}")
        return False
    
    try:
        embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        db = FAISS.load_local(VECTORSTORE_PATH, embeddings=embedding, allow_dangerous_deserialization=True)
        print("‚úÖ Vector store loaded successfully")
        return True
    except Exception as e:
        print(f"‚ùå Error loading vector store: {e}")
        return False

def get_answer_from_repo(question):
    global db
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π
    repo_path = load_repo_state()
    if not repo_path:
        return "‚ùå No repository loaded. Please load a repository first."
    
    # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è —â–æ –≤–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞
    if not ensure_db_loaded():
        return "‚ùå Failed to load vector database. Please reload the repository."

    print("üü¢ RAG answering question:", question)
    try:
        llm = get_openrouter_llm()
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=db.as_retriever(),
            chain_type_kwargs={"verbose": True}
        )
        return qa.run(question)
    except Exception as e:
        print(f"‚ùå Error in get_answer_from_repo: {e}")
        return f"Error generating answer: {str(e)}"

def get_answer_from_repo_streaming(question):
    """–°—Ç—Ä—ñ–º—ñ–Ω–≥–æ–≤–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π"""
    global db
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π
    repo_path = load_repo_state()
    if not repo_path:
        yield "‚ùå No repository loaded. Please load a repository first."
        return
    
    # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è —â–æ –≤–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞
    if not ensure_db_loaded():
        yield "‚ùå Failed to load vector database. Please reload the repository."
        return

    print("üü¢ RAG answering question with streaming:", question)
    
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        retriever = db.as_retriever(search_kwargs={"k": 4})
        docs = retriever.get_relevant_documents(question)
        
        if not docs:
            yield "No relevant documents found in the repository."
            return
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç
        prompt = f"""Based on the following code repository context, please answer the question.

Context from repository:
{context}

Question: {question}

Please provide a detailed and helpful answer based on the code context above."""

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç—Ä—ñ–º—ñ–Ω–≥–æ–≤—É –º–æ–¥–µ–ª—å
        llm = get_openrouter_llm_streaming()
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö
        for chunk in llm.stream([{"role": "user", "content": prompt}]):
            yield chunk
            
    except Exception as e:
        print(f"‚ùå Error in get_answer_from_repo_streaming: {e}")
        yield f"Error generating response: {str(e)}"